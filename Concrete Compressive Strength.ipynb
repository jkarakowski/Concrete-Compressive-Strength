{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exploring Neural Networks using [Concrete Compressive Strength Set from UCI](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)\n",
    "\n",
    "#### Analysis follows Chapter 7 of *Machine Learning with R* by Brett Lantz (though of course here we use Python, not R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:  Use a neural network to predict the compressive strength of concrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first use the MLPRegressor function provided by scikit-learn, then use Keras to create a customized neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('Concrete_Data.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['Cement','Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer', 'Coarse Aggregate', \n",
    "                'Fine Aggregate', 'Age', 'Compressive Strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Compressive Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.887366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.269535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.052780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.296075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Compressive Strength  \n",
       "0            1040.0           676.0   28             79.986111  \n",
       "1            1055.0           676.0   28             61.887366  \n",
       "2             932.0           594.0  270             40.269535  \n",
       "3             932.0           594.0  365             41.052780  \n",
       "4             978.4           825.5  360             44.296075  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cement</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>281.165631</td>\n",
       "      <td>104.507142</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>192.375000</td>\n",
       "      <td>272.900000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>73.895485</td>\n",
       "      <td>86.279104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>359.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fly Ash</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>54.187136</td>\n",
       "      <td>63.996469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.270000</td>\n",
       "      <td>200.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>181.566359</td>\n",
       "      <td>21.355567</td>\n",
       "      <td>121.750000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>247.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Superplasticizer</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>6.203112</td>\n",
       "      <td>5.973492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>32.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>972.918592</td>\n",
       "      <td>77.753818</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>1145.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>773.578883</td>\n",
       "      <td>80.175427</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>779.510000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>992.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Compressive Strength</th>\n",
       "      <td>1030.0</td>\n",
       "      <td>35.817836</td>\n",
       "      <td>16.705679</td>\n",
       "      <td>2.331808</td>\n",
       "      <td>23.707115</td>\n",
       "      <td>34.442774</td>\n",
       "      <td>46.136287</td>\n",
       "      <td>82.599225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count        mean         std         min         25%  \\\n",
       "Cement                1030.0  281.165631  104.507142  102.000000  192.375000   \n",
       "Blast Furnace Slag    1030.0   73.895485   86.279104    0.000000    0.000000   \n",
       "Fly Ash               1030.0   54.187136   63.996469    0.000000    0.000000   \n",
       "Water                 1030.0  181.566359   21.355567  121.750000  164.900000   \n",
       "Superplasticizer      1030.0    6.203112    5.973492    0.000000    0.000000   \n",
       "Coarse Aggregate      1030.0  972.918592   77.753818  801.000000  932.000000   \n",
       "Fine Aggregate        1030.0  773.578883   80.175427  594.000000  730.950000   \n",
       "Age                   1030.0   45.662136   63.169912    1.000000    7.000000   \n",
       "Compressive Strength  1030.0   35.817836   16.705679    2.331808   23.707115   \n",
       "\n",
       "                             50%          75%          max  \n",
       "Cement                272.900000   350.000000   540.000000  \n",
       "Blast Furnace Slag     22.000000   142.950000   359.400000  \n",
       "Fly Ash                 0.000000   118.270000   200.100000  \n",
       "Water                 185.000000   192.000000   247.000000  \n",
       "Superplasticizer        6.350000    10.160000    32.200000  \n",
       "Coarse Aggregate      968.000000  1029.400000  1145.000000  \n",
       "Fine Aggregate        779.510000   824.000000   992.600000  \n",
       "Age                    28.000000    56.000000   365.000000  \n",
       "Compressive Strength   34.442774    46.136287    82.599225  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['Compressive Strength']\n",
    "X = data.drop(labels = ['Compressive Strength'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's start by following the exercise in Lantz:\n",
    "- scale the data to be in [0, 1]\n",
    "- train the neural net with only 1 node (otherwise we'll use the default parameters)\n",
    "- check the correlation between the predictions and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7234)\n",
    "MMS = MinMaxScaler()\n",
    "MMS.fit(X_train)\n",
    "X_train = MMS.transform(X_train)\n",
    "X_test = MMS.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = MLPRegressor(hidden_layer_sizes=(1,))\n",
    "NN.fit(X_train, y_train)\n",
    "predictions = NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.n_outputs_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation = 0.407342952661\n"
     ]
    }
   ],
   "source": [
    "print('correlation =', pearsonr(predictions, y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score = -2.17568586835\n"
     ]
    }
   ],
   "source": [
    "print('R2 score =',NN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this net doesn't do a very good job.  Let's create a pipeline and try varying the parameters to find one that works:\n",
    "- different scalers\n",
    "- different # of nodes/layers\n",
    "- different activation functions\n",
    "- different algorithms\n",
    "- different weights (alpha) for the l2 correction term\n",
    "\n",
    "We'll use R2 score as the accuracy measure.  Correlation will be used as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler to be used in the grid search\n",
    "scaler_list = [\n",
    "    ('mm', MinMaxScaler(), {})\n",
    "    ,('std', StandardScaler(), {})\n",
    "\n",
    "]\n",
    "#parameters for the MLP regressor to be used in the grid search\n",
    "classifier_list = [\n",
    "    ('mlp', MLPRegressor(), {'mlp__hidden_layer_sizes': ((10,),(10, 10),(10, 10, 10), (30,),(30, 30),(30, 30, 30),\n",
    "                                                        (50,),(50, 50),(50, 50, 50),(100,),(100, 100),(100, 100, 100)),        \n",
    "    'mlp__activation':('logistic','relu','identity','tanh'),\n",
    "     'mlp__solver':('lbfgs','sgd','adam'),\n",
    "    'mlp__alpha':(1.0e-4,1.0e-3,1.0e-2,1.0e-1,1.0, 1.0e+1, 1.0e+2)})\n",
    "]\n",
    "results = {}\n",
    "\n",
    "#loop over all scalers (2) and classifiers (only 1 here)\n",
    "for s in scaler_list:\n",
    "    for c in classifier_list:\n",
    "        \n",
    "        pipeline = Pipeline([ (s[0], s[1]), (c[0], c[1]) ])\n",
    "\n",
    "        parameters = {}\n",
    "        parameters.update(s[2])\n",
    "        parameters.update(c[2])\n",
    "        \n",
    "        #do a grid search on the current scaler and classifier\n",
    "        grid_search = GridSearchCV(pipeline, parameters, scoring='r2', verbose=0, n_jobs=4)\n",
    "\n",
    "        results[(s[0], c[0])] = grid_search.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2 for MinMax scaling:  0.86933232991\n"
     ]
    }
   ],
   "source": [
    "print('Best R2 for MinMax scaling: ', results[('mm', 'mlp')].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2 for Z-scaling:  0.882166511735\n"
     ]
    }
   ],
   "source": [
    "print('Best R2 for Z-scaling: ',results[('std', 'mlp')].best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better.  Let's see which parameters give the best test scores.  First, MinMax scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_mlp__activation</th>\n",
       "      <th>param_mlp__alpha</th>\n",
       "      <th>param_mlp__hidden_layer_sizes</th>\n",
       "      <th>param_mlp__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.455612</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.869332</td>\n",
       "      <td>0.939832</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858329</td>\n",
       "      <td>0.944136</td>\n",
       "      <td>0.879283</td>\n",
       "      <td>0.931194</td>\n",
       "      <td>0.870432</td>\n",
       "      <td>0.944167</td>\n",
       "      <td>0.020130</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.006108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>0.310932</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.866853</td>\n",
       "      <td>0.928102</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851582</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>0.876704</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.872337</td>\n",
       "      <td>0.937716</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>0.010976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1.547511</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.864609</td>\n",
       "      <td>0.921997</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 0.0001,...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.844834</td>\n",
       "      <td>0.927067</td>\n",
       "      <td>0.878915</td>\n",
       "      <td>0.911406</td>\n",
       "      <td>0.870160</td>\n",
       "      <td>0.927517</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.014460</td>\n",
       "      <td>0.007491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.363346</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.863960</td>\n",
       "      <td>0.927748</td>\n",
       "      <td>logistic</td>\n",
       "      <td>1</td>\n",
       "      <td>(30, 30)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.831027</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.883401</td>\n",
       "      <td>0.923687</td>\n",
       "      <td>0.877590</td>\n",
       "      <td>0.933857</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.023456</td>\n",
       "      <td>0.004397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0.219124</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.861604</td>\n",
       "      <td>0.920326</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1</td>\n",
       "      <td>(30,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.842356</td>\n",
       "      <td>0.924312</td>\n",
       "      <td>0.876408</td>\n",
       "      <td>0.906523</td>\n",
       "      <td>0.866127</td>\n",
       "      <td>0.930142</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.010046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "927       0.455612         0.001282         0.869332          0.939832   \n",
       "918       0.310932         0.000968         0.866853          0.928102   \n",
       "282       1.547511         0.001588         0.864609          0.921997   \n",
       "156       0.363346         0.000927         0.863960          0.927748   \n",
       "909       0.219124         0.000825         0.861604          0.920326   \n",
       "\n",
       "    param_mlp__activation param_mlp__alpha param_mlp__hidden_layer_sizes  \\\n",
       "927                  tanh                1                        (100,)   \n",
       "918                  tanh                1                         (50,)   \n",
       "282                  relu           0.0001                    (100, 100)   \n",
       "156              logistic                1                      (30, 30)   \n",
       "909                  tanh                1                         (30,)   \n",
       "\n",
       "    param_mlp__solver                                             params  \\\n",
       "927             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...   \n",
       "918             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...   \n",
       "282             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 0.0001,...   \n",
       "156             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...   \n",
       "909             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 1.0, 'm...   \n",
       "\n",
       "     rank_test_score  split0_test_score  split0_train_score  \\\n",
       "927                1           0.858329            0.944136   \n",
       "918                2           0.851582            0.933849   \n",
       "282                3           0.844834            0.927067   \n",
       "156                4           0.831027            0.925701   \n",
       "909                5           0.842356            0.924312   \n",
       "\n",
       "     split1_test_score  split1_train_score  split2_test_score  \\\n",
       "927           0.879283            0.931194           0.870432   \n",
       "918           0.876704            0.912742           0.872337   \n",
       "282           0.878915            0.911406           0.870160   \n",
       "156           0.883401            0.923687           0.877590   \n",
       "909           0.876408            0.906523           0.866127   \n",
       "\n",
       "     split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "927            0.944167      0.020130        0.000084        0.008593   \n",
       "918            0.937716      0.011839        0.000075        0.010966   \n",
       "282            0.927517      0.006709        0.000040        0.014460   \n",
       "156            0.933857      0.003942        0.000008        0.023456   \n",
       "909            0.930142      0.015505        0.000031        0.014269   \n",
       "\n",
       "     std_train_score  \n",
       "927         0.006108  \n",
       "918         0.010976  \n",
       "282         0.007491  \n",
       "156         0.004397  \n",
       "909         0.010046  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = pd.DataFrame(results[('mm', 'mlp')].cv_results_)\n",
    "rdf.sort_values(by='mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_mlp__activation</th>\n",
       "      <th>param_mlp__alpha</th>\n",
       "      <th>param_mlp__hidden_layer_sizes</th>\n",
       "      <th>param_mlp__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2.500739</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.882167</td>\n",
       "      <td>0.971690</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>(100, 100, 100)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856028</td>\n",
       "      <td>0.971337</td>\n",
       "      <td>0.901952</td>\n",
       "      <td>0.974361</td>\n",
       "      <td>0.888629</td>\n",
       "      <td>0.969373</td>\n",
       "      <td>0.034118</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.002052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.423166</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.881758</td>\n",
       "      <td>0.968023</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 10.0, '...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.859192</td>\n",
       "      <td>0.970434</td>\n",
       "      <td>0.891540</td>\n",
       "      <td>0.965678</td>\n",
       "      <td>0.894637</td>\n",
       "      <td>0.967958</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.509851</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.881330</td>\n",
       "      <td>0.958858</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>(30, 30, 30)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857275</td>\n",
       "      <td>0.961008</td>\n",
       "      <td>0.891072</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.895741</td>\n",
       "      <td>0.961484</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>0.322067</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.879919</td>\n",
       "      <td>0.961848</td>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 10.0, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861349</td>\n",
       "      <td>0.966423</td>\n",
       "      <td>0.895001</td>\n",
       "      <td>0.959248</td>\n",
       "      <td>0.883485</td>\n",
       "      <td>0.959873</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.013972</td>\n",
       "      <td>0.003245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>1.201638</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.877526</td>\n",
       "      <td>0.966228</td>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>(50, 50, 50)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.845841</td>\n",
       "      <td>0.970947</td>\n",
       "      <td>0.897508</td>\n",
       "      <td>0.963460</td>\n",
       "      <td>0.889360</td>\n",
       "      <td>0.964277</td>\n",
       "      <td>0.056364</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.003353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "501       2.500739         0.001932         0.882167          0.971690   \n",
       "963       0.423166         0.001249         0.881758          0.968023   \n",
       "483       0.509851         0.000998         0.881330          0.958858   \n",
       "954       0.322067         0.001043         0.879919          0.961848   \n",
       "492       1.201638         0.001326         0.877526          0.966228   \n",
       "\n",
       "    param_mlp__activation param_mlp__alpha param_mlp__hidden_layer_sizes  \\\n",
       "501                  relu              100               (100, 100, 100)   \n",
       "963                  tanh               10                        (100,)   \n",
       "483                  relu              100                  (30, 30, 30)   \n",
       "954                  tanh               10                         (50,)   \n",
       "492                  relu              100                  (50, 50, 50)   \n",
       "\n",
       "    param_mlp__solver                                             params  \\\n",
       "501             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...   \n",
       "963             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 10.0, '...   \n",
       "483             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...   \n",
       "954             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 10.0, '...   \n",
       "492             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 100.0, ...   \n",
       "\n",
       "     rank_test_score  split0_test_score  split0_train_score  \\\n",
       "501                1           0.856028            0.971337   \n",
       "963                2           0.859192            0.970434   \n",
       "483                3           0.857275            0.961008   \n",
       "954                4           0.861349            0.966423   \n",
       "492                5           0.845841            0.970947   \n",
       "\n",
       "     split1_test_score  split1_train_score  split2_test_score  \\\n",
       "501           0.901952            0.974361           0.888629   \n",
       "963           0.891540            0.965678           0.894637   \n",
       "483           0.891072            0.954082           0.895741   \n",
       "954           0.895001            0.959248           0.883485   \n",
       "492           0.897508            0.963460           0.889360   \n",
       "\n",
       "     split2_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "501            0.969373      0.034118        0.000159        0.019302   \n",
       "963            0.967958      0.015636        0.000079        0.016040   \n",
       "483            0.961484      0.016881        0.000025        0.017150   \n",
       "954            0.959873      0.014220        0.000056        0.013972   \n",
       "492            0.964277      0.056364        0.000017        0.022696   \n",
       "\n",
       "     std_train_score  \n",
       "501         0.002052  \n",
       "963         0.001942  \n",
       "483         0.003383  \n",
       "954         0.003245  \n",
       "492         0.003353  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = pd.DataFrame(results[('std', 'mlp')].cv_results_)\n",
    "rdf.sort_values(by='mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest scores are for Z-scaled data, the lbfgs algorithm, and the relu function with higher alpha.  Higher alphas seem to require more hidden layers, while lower alphas require fewer.  At the risk of overfitting, let's try to come up with even better values, using alphas between 1 and 100, and varying the number of nodes, keeping only one or 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_list = [\n",
    "    ('std', StandardScaler(), {})\n",
    "]\n",
    "classifier_list = [\n",
    "    ('mlp', MLPRegressor(), {'mlp__hidden_layer_sizes': ((30,),(30, 30), (40,),(40, 40), (50,),(50, 50),\n",
    "                                                        (60,),(60, 60),(70,),(70, 70),(80,), (80, 80),\n",
    "                                                        (90,),(90, 90),(100,),(100, 100)),        \n",
    "    'mlp__activation':('relu',),\n",
    "    'mlp__solver':('lbfgs',),\n",
    "    'mlp__alpha':(1, 5, 10, 20, 50, 100)})\n",
    "]\n",
    "results = {}\n",
    "\n",
    "for s in scaler_list:\n",
    "    for c in classifier_list:\n",
    "        \n",
    "        pipeline = Pipeline([ (s[0], s[1]), (c[0], c[1]) ])\n",
    "\n",
    "        parameters = {}\n",
    "        parameters.update(s[2])\n",
    "        parameters.update(c[2])\n",
    "\n",
    "        grid_search = GridSearchCV(pipeline, parameters, scoring='r2', verbose=0, n_jobs=4)\n",
    "\n",
    "        results[(s[0], c[0])] = grid_search.fit(X_train,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_mlp__activation</th>\n",
       "      <th>param_mlp__alpha</th>\n",
       "      <th>param_mlp__hidden_layer_sizes</th>\n",
       "      <th>param_mlp__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.612897</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.961319</td>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>(50, 50)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 50, 'ml...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851039</td>\n",
       "      <td>0.960029</td>\n",
       "      <td>0.905638</td>\n",
       "      <td>0.962458</td>\n",
       "      <td>0.887546</td>\n",
       "      <td>0.961471</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.908198</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.880527</td>\n",
       "      <td>0.982358</td>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 10, 'ml...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.844885</td>\n",
       "      <td>0.983387</td>\n",
       "      <td>0.893745</td>\n",
       "      <td>0.981364</td>\n",
       "      <td>0.903099</td>\n",
       "      <td>0.982323</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.025541</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.367274</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.880134</td>\n",
       "      <td>0.968935</td>\n",
       "      <td>relu</td>\n",
       "      <td>1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 1, 'mlp...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857414</td>\n",
       "      <td>0.969064</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.970277</td>\n",
       "      <td>0.895641</td>\n",
       "      <td>0.967462</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.016442</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.525953</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.878272</td>\n",
       "      <td>0.980228</td>\n",
       "      <td>relu</td>\n",
       "      <td>5</td>\n",
       "      <td>(90, 90)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 5, 'mlp...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.982050</td>\n",
       "      <td>0.903341</td>\n",
       "      <td>0.979508</td>\n",
       "      <td>0.889496</td>\n",
       "      <td>0.979125</td>\n",
       "      <td>0.090760</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.001298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.547184</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.878164</td>\n",
       "      <td>0.967602</td>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>(90, 90)</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'mlp__solver': 'lbfgs', 'mlp__alpha': 50, 'ml...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.848060</td>\n",
       "      <td>0.969957</td>\n",
       "      <td>0.888188</td>\n",
       "      <td>0.966420</td>\n",
       "      <td>0.898369</td>\n",
       "      <td>0.966430</td>\n",
       "      <td>0.040029</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>0.001665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "69       0.612897         0.001081         0.881365          0.961319   \n",
       "47       1.908198         0.001661         0.880527          0.982358   \n",
       "14       0.367274         0.001179         0.880134          0.968935   \n",
       "29       1.525953         0.001761         0.878272          0.980228   \n",
       "77       1.547184         0.001456         0.878164          0.967602   \n",
       "\n",
       "   param_mlp__activation param_mlp__alpha param_mlp__hidden_layer_sizes  \\\n",
       "69                  relu               50                      (50, 50)   \n",
       "47                  relu               10                    (100, 100)   \n",
       "14                  relu                1                        (100,)   \n",
       "29                  relu                5                      (90, 90)   \n",
       "77                  relu               50                      (90, 90)   \n",
       "\n",
       "   param_mlp__solver                                             params  \\\n",
       "69             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 50, 'ml...   \n",
       "47             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 10, 'ml...   \n",
       "14             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 1, 'mlp...   \n",
       "29             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 5, 'mlp...   \n",
       "77             lbfgs  {'mlp__solver': 'lbfgs', 'mlp__alpha': 50, 'ml...   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "69                1           0.851039            0.960029           0.905638   \n",
       "47                2           0.844885            0.983387           0.893745   \n",
       "14                3           0.857414            0.969064           0.887441   \n",
       "29                4           0.842130            0.982050           0.903341   \n",
       "77                5           0.848060            0.969957           0.888188   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "69            0.962458           0.887546            0.961471      0.007971   \n",
       "47            0.981364           0.903099            0.982323      0.025905   \n",
       "14            0.970277           0.895641            0.967462      0.007989   \n",
       "29            0.979508           0.889496            0.979125      0.090760   \n",
       "77            0.966420           0.898369            0.966430      0.040029   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "69        0.000007        0.022721         0.000997  \n",
       "47        0.000272        0.025541         0.000826  \n",
       "14        0.000306        0.016442         0.001153  \n",
       "29        0.000406        0.026225         0.001298  \n",
       "77        0.000011        0.021732         0.001665  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = pd.DataFrame(results[('std', 'mlp')].cv_results_)\n",
    "rdf.sort_values(by='mean_test_score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values are alpha=50 and two hidden layers of 50 nodes each.  Let's see how this looks on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation = 0.95795267284\n",
      "R2 score = 0.917300471449\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7234)\n",
    "ZS = StandardScaler()\n",
    "ZS.fit(X_train)\n",
    "X_train = ZS.transform(X_train)\n",
    "X_test = ZS.transform(X_test)\n",
    "NN = MLPRegressor(hidden_layer_sizes=(50, 50), solver='lbfgs', alpha=50, activation='relu')\n",
    "#NN = MLPRegressor(hidden_layer_sizes=(30,30), solver='lbfgs', alpha=10, activation='relu')\n",
    "NN.fit(X_train, y_train)\n",
    "predictions = NN.predict(X_test)\n",
    "print('correlation =', pearsonr(predictions, y_test)[0])\n",
    "print('R2 score =',NN.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite nice.  And visually, the points lie fairly close to the 45 degree line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEFScCARBEAQVJwJBEARBxfkB430/4UxXt+MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112a54240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "plt.scatter(y_test, predictions, marker='o', color='g')\n",
    "plt.plot((0,80),(0,80), color='r')\n",
    "plt.xlabel('Actual test compression strength')\n",
    "plt.ylabel('Predicted test compression strength')\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, let's try creating a neural net in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard scaler had the best performance above, so let's just stick with that\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7234)\n",
    "SS = StandardScaler()\n",
    "SS.fit(X_train)\n",
    "X_train = SS.transform(X_train)\n",
    "X_test = SS.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#customized metric - doesn't already exist ?!?\n",
    "def r_squared(y_true, y_pred):\n",
    "    ''' Customized loss function for R^2 \n",
    "    y_true = Keras tensor of actual values\n",
    "    y_pred = Keras tensor of predicted values\n",
    "    \n",
    "    '''\n",
    "    mu = K.mean(y_true)\n",
    "    tss = K.sum(K.square(y_true - mu)) #total sum of squares\n",
    "    rss = K.sum(K.square(y_true - y_pred)) #residual sum of squares\n",
    "\n",
    "    return (1 - rss/(K.epsilon() + tss))\n",
    "\n",
    "#each hidden layer can be a different size\n",
    "def nn_model(layers, activation, optimizer, input_shape, alpha=1):\n",
    "    '''Neural net model of only dense layers\n",
    "\n",
    "    layers: List of integers, the number of hidden units in each layer\n",
    "    activation:  String, name of activation function used for each layer ('relu', etc.)\n",
    "    optimizer:  String, name of optimization algorithm ('sgd', etc.)\n",
    "    input_shape:  Sequence of integers, dimensions of the input layer\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "   \n",
    "    first_layer = True\n",
    "\n",
    "    for layer in layers:\n",
    "        #if 1st layer, need to specify input shape\n",
    "        if first_layer:\n",
    "            model.add(Dense(layer, input_shape = input_shape, kernel_regularizer=regularizers.l2(alpha)) )\n",
    "            first_layer = False\n",
    "        else:\n",
    "            model.add(Dense(layer, kernel_regularizer=regularizers.l2(alpha)))\n",
    "\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[r_squared])\n",
    "\n",
    "    return model\n",
    "\n",
    "#each hidden layer is the same size\n",
    "def nn_model_same_size(layer_size, no_layers, activation, optimizer, input_shape, alpha=1):\n",
    "    '''Neural net model of only dense layers\n",
    "\n",
    "    layer_size: Integer, the number of hidden units in each layer\n",
    "    no_layers: Integer, the number of hidden layers\n",
    "    activation:  String, name of activation function used for each layer ('relu', etc.)\n",
    "    optimizer:  String, name of optimization algorithm ('sgd', etc.)\n",
    "    input_shape:  Sequence of integers, dimensions of the input layer\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for layer in range(no_layers):\n",
    "        #if 1st layer, need to specify input shape\n",
    "        if layer:\n",
    "            model.add(Dense(layer_size, kernel_regularizer=regularizers.l2(alpha)))\n",
    "        else:\n",
    "            model.add(Dense(layer_size, input_shape = input_shape, kernel_regularizer=regularizers.l2(alpha)) )\n",
    "            \n",
    "        model.add(Activation(activation))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[r_squared])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309/309 [==============================] - 0s 100us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[31.176885783865227, 0.89062236967981823]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel = nn_model_same_size(50, 2, 'tanh', 'sgd', (X_train.shape[1],), alpha=0)\n",
    "myModel.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "myModel.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's pretty close to the above result.  Let's set up a grid search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this post on using GridSearchCV with keras:\n",
    "#    https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "keras_model = KerasRegressor(build_fn=nn_model_same_size, input_shape=(X_train.shape[1],), alpha=0)\n",
    "layer_list = (10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n",
    "layer_depth = (1, 2)\n",
    "\n",
    "grid_search = GridSearchCV(keras_model\n",
    "                         ,param_grid={'layer_size': layer_list\n",
    "                                      ,'no_layers': layer_depth\n",
    "                                     ,'activation': ('tanh',)\n",
    "                                     ,'optimizer': ('sgd',)\n",
    "                                     } \n",
    "                        ,scoring='r2'\n",
    "                        \n",
    "                        )\n",
    "result = grid_search.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961116 +/- 0.023201 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 100}\n",
      "0.959955 +/- 0.006217 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 50}\n",
      "0.958119 +/- 0.032429 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 80}\n",
      "0.954299 +/- 0.023271 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 50}\n",
      "0.953913 +/- 0.012453 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 100}\n",
      "0.950023 +/- 0.023831 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 60}\n",
      "0.946878 +/- 0.020900 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 20}\n",
      "0.943221 +/- 0.028370 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 40}\n",
      "0.940524 +/- 0.023282 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 90}\n",
      "0.938671 +/- 0.035629 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 90}\n",
      "0.935409 +/- 0.059755 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 30}\n",
      "0.928155 +/- 0.015710 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 20}\n",
      "0.921532 +/- 0.047624 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 80}\n",
      "0.916349 +/- 0.034581 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 30}\n",
      "0.909157 +/- 0.042804 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 70}\n",
      "0.904803 +/- 0.086697 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 70}\n",
      "0.896983 +/- 0.019837 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 10}\n",
      "0.833096 +/- 0.085615 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 10}\n",
      "0.815294 +/- 0.201848 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 40}\n",
      "0.549597 +/- 0.605130 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 60}\n"
     ]
    }
   ],
   "source": [
    "means = result.cv_results_['mean_train_score']\n",
    "stds = result.cv_results_['std_train_score']\n",
    "params = result.cv_results_['params']\n",
    "#display results in order of mean r squared\n",
    "for i in np.argsort(means)[::-1]:\n",
    "   print('%f +/- %f with: %r' % (means[i], stds[i], params[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the bigger the better.  Let's see the test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864078 +/- 0.023153 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 50}\n",
      "0.859458 +/- 0.020838 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 100}\n",
      "0.858371 +/- 0.024622 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 60}\n",
      "0.856091 +/- 0.013983 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 90}\n",
      "0.838957 +/- 0.018597 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 80}\n",
      "0.837682 +/- 0.005423 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 100}\n",
      "0.833136 +/- 0.039102 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 20}\n",
      "0.830775 +/- 0.021695 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 70}\n",
      "0.829430 +/- 0.011959 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 10}\n",
      "0.829155 +/- 0.042371 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 30}\n",
      "0.827802 +/- 0.026931 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 80}\n",
      "0.821802 +/- 0.008474 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 50}\n",
      "0.816145 +/- 0.022180 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 30}\n",
      "0.807294 +/- 0.014186 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 40}\n",
      "0.801221 +/- 0.022660 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 90}\n",
      "0.800978 +/- 0.013695 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 20}\n",
      "0.778068 +/- 0.088319 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 70}\n",
      "0.722021 +/- 0.038963 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 10}\n",
      "0.682266 +/- 0.263433 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 1, 'layer_size': 40}\n",
      "0.434758 +/- 0.550688 with: {'optimizer': 'sgd', 'activation': 'tanh', 'no_layers': 2, 'layer_size': 60}\n"
     ]
    }
   ],
   "source": [
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "\n",
    "for i in np.argsort(means)[::-1]:\n",
    "   print('%f +/- %f with: %r' % (means[i], stds[i], params[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so maybe the larger networks are overfitting.  Let's use a single 50-unit hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try varying alpha to reduce overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = KerasRegressor(build_fn=nn_model_same_size, input_shape=(X_train.shape[1],))\n",
    "\n",
    "alpha_list = (0, 0.0001, 0.001, 0.01, 0.1, 1)\n",
    "\n",
    "grid_search = GridSearchCV(keras_model\n",
    "                         ,param_grid={'layer_size': (50,)\n",
    "                                      ,'no_layers': (1,)\n",
    "                                     ,'activation': ('tanh', )\n",
    "                                     ,'optimizer': ('sgd',)\n",
    "                                    ,'alpha': alpha_list\n",
    "                                     } \n",
    "                        ,scoring='r2'\n",
    "                        \n",
    "                        )\n",
    "result = grid_search.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862485 +/- 0.013195 with: {'alpha': 0.0001, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n",
      "0.860817 +/- 0.019792 with: {'alpha': 0.001, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n",
      "0.851777 +/- 0.018501 with: {'alpha': 0, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n",
      "0.845669 +/- 0.009952 with: {'alpha': 0.01, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n",
      "0.816843 +/- 0.052016 with: {'alpha': 0.1, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n",
      "0.382342 +/- 0.189443 with: {'alpha': 1, 'activation': 'tanh', 'optimizer': 'sgd', 'no_layers': 1, 'layer_size': 50}\n"
     ]
    }
   ],
   "source": [
    "means = result.cv_results_['mean_test_score']\n",
    "stds = result.cv_results_['std_test_score']\n",
    "params = result.cv_results_['params']\n",
    "\n",
    "for i in np.argsort(means)[::-1]:\n",
    "   print('%f +/- %f with: %r' % (means[i], stds[i], params[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, the smallest values of alpha are within the errors of the best R squared value, so that doesn't seem any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try with the real test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89563490158806136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best R squared value found using MLPRegressor was 0.9173.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
